{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ed6301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos que habíamos guardado del entrenamiento\n",
    "# Estos datos ya se encuentran estandarizados y con los atributos seleccionados\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Matriz de características con la clase\n",
    "train = np.load('../fianl_features/train.npy')\n",
    "\n",
    "# Ahora vamos a separar los atributos de la clase\n",
    "\n",
    "X_train = train[:, :-1]\n",
    "y_train = train[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b228bd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los modelos de clasificación\n",
    "\n",
    "# Utilizaremos el modelo del Perceptron Multicapa\n",
    "# Este modelo nos permite clasificar datos de manera no lineal\n",
    "# Es un algoritmo que se basa en el uso de redes neuronales\n",
    "# y lo que hace es buscar conexiones entre las neuronas durante el forward propagation\n",
    "# evalua el error cometido (nivel de exactitud alcanzado)\n",
    "# y en base a eso, durante el backward propagation actualiza los hiperparametros de las conexiones de las neuronas\n",
    "# para en la siguiente iteración reducir el error o aumentar el nivel de exactitud durante la predicción\n",
    "# Es un algoritmo iterativo\n",
    "# Los algoritmos de Perceptron se pueden usar tanto para regresión como para clasificación\n",
    "\n",
    "# Así mismo también utilizaremos un modelo de Logistic Regression para comparar los resultados obtenidos\n",
    "# No confundir Logistic Regression con Regresión lineal. Logistic Regression se utiliza para problemas de clasificación\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "algortimos = {\n",
    "    'LOGR': LogisticRegression(penalty='l2', solver='saga', max_iter=1000, random_state=42),\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=[8, 4], activation='relu', solver='sgd', batch_size='auto', \n",
    "                         learning_rate='constant', learning_rate_initial=0.01, max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Cross-validation iterno en k=5 bolsas. Esto nos permitira mejorar los hiperparametros de nustros\n",
    "# algoritmos para obtener un mejor resultado\n",
    "# Dividiremos en dos sub conjuntos nuestro conjunto de entrenamiento.\n",
    "# Uno será de entrenamiento y otro de validacion.\n",
    "\n",
    "# Con el metodo cross_val_score lo que hacemos es obtener el accuracy de nuestro modelo\n",
    "# para cada una de las bolsas. Esto nos ayudara a observar que tan robusto es nuestro modelo\n",
    "# ya que en caso de hacer una única partición, no sabemos si nuestro modelo es lo suficiente robusto por \n",
    "# los datos escogidos, o si es robusto en general. El metodo cross_val_score lo que hace es agregar\n",
    "# aleatoridad\n",
    "\n",
    "# Obtenemos el accuracy en cada una de las 5 bolsas de validación\n",
    "\n",
    "results = {}\n",
    "\n",
    "# El accuracy es un buen indicador ya que nos muestra el porcentaje de muestras que estoy acertando\n",
    "# con respecto al total\n",
    "for nombre, alg in algoritmos.items():\n",
    "    results[nombre] = cross_val_score(alg, X_train, y_train, cv=KFold(n_splits=5, shuffle=True, random_state=42))\n",
    "    print(nombre + ': Accuracy: %0.4f +/- %0.4f' % (results[nombre].mean(), results[nombre].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85790af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una vez que ya hemos determinado los hiperparametros y ya no los podemos optimizar todavia mas\n",
    "# ahora si ya podemos definir el modelo definitivo\n",
    "\n",
    "algortimos = {\n",
    "    'LOGR': LogisticRegression(penalty='l2', solver='saga', max_iter=1000, random_state=42),\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=[8, 4], activation='relu', solver='sgd', batch_size='auto', \n",
    "                         learning_rate='constant', learning_rate_initial=0.01, max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "LOGR_definitivo = LOGR.fit(X_train, y_train)\n",
    "MLP_definitivo = MLP.fit(X_train, y_train)\n",
    "\n",
    "# Atributos que se obtienen durante el entrenamiento\n",
    "\n",
    "# El error mínimo cometido durante las etapas de entrnamiento\n",
    "print('Minimo error cometido: ', MLP_definitivo.best_loss_)\n",
    "print('Numero de iteraciones llevadas a cabo: ', MLP_definitivo.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebacca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una vez que ya hemos entrenado los modelos con los datos de ENTRENAMIENTO\n",
    "# lo que haremos será guardar los modelos en nuestra computadora\n",
    "# para poder utilizarlos en un futuro con los datos de test\n",
    "\n",
    "import os\n",
    "impoer pickle\n",
    "\n",
    "if os.path.exists('../models'):\n",
    "    os.mkdir('../models')\n",
    "    \n",
    "with open('../models/LOGR.pickle', 'wb') as fw:\n",
    "    pickle.dump(LOGR_definitivo, fw)\n",
    "    \n",
    "with open('../models/MLP.pickle', 'wb') as fw:\n",
    "    pickle.dump(MLP_definitivo, fw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
