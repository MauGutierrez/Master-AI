{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61990fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos de entrenamiento\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train_matrix = np.load('../features/matriz_datos_train.npy')\n",
    "\n",
    "# Selecionar las features y target, es decir, separar los Atributos de la clase\n",
    "# donde la matriz X será la matríz de Atributos y el vector Y será el vector de la clase\n",
    "\n",
    "# Atributos\n",
    "X_train = train_matrix[:, :-1]\n",
    "\n",
    "# Clase\n",
    "y_train = train_matrix[:, -1]\n",
    "\n",
    "# Eliminaremos atributos que no contribuyen a nuestro modelo y solamente generan ruido\n",
    "\n",
    "# Dependiendo del modelo que seguimos, podemos aplicar antes o después la estandarización\n",
    "# En clase vimos el método del variance Threshold que lo que hace es eliminar los atributos que \n",
    "# no tienen una varianza muy grande (dependiendo del parametro que nosotros pongamos)\n",
    "\n",
    "# Sin embargo, para este metodo lo que haremos primero será aplicar la estandarización\n",
    "# Ya estandarizados analizaremos la distribución que siguen esas variables\n",
    "# y en base a la distribución aplicaremos una selección de atributos u otra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcc71f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estandarización de los datos de entrenamiento. Los de test de momento los dejamos de lado\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "estandarizador = StandardScaler()\n",
    "\n",
    "# Ya entrenado el objeto estandarizador, puedo extraer las variables que son importantes en esta funcion\n",
    "estandarizador.fit(X_train)\n",
    "\n",
    "# Lo que hace el metodo StandardScaler es coger cada uno de los valores para una columna\n",
    "# y le resta a cada valor de la columna la media de dicha columna y lo divide entre la desviación\n",
    "# típica de esa columna\n",
    "\n",
    "# Hace un centrado y un escalado en base a la media y la desviación estandar\n",
    "\n",
    "mu = estandarizador.mean_\n",
    "sigma = np.sqrt(estandarizador.var_)\n",
    "\n",
    "# Conjunto de datos ya estandarizado\n",
    "X_train = estandarizador.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee500d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de los atributos o características\n",
    "\n",
    "# Primero estudiaremos la normalidad de los datos para saber si siguen una distribución normal\n",
    "# Si siguen una distribución normal haremos una comparación de media\n",
    "# y en caso contrario haremos una comparación de la mediana\n",
    "\n",
    "# Estudiar si las variables siguen una distribución normal de media 0 y desviación típica 1 ---> N(0, 1)\n",
    "# Kstest hace referencia a la prueba de Kolmogorov-Smirnov\n",
    "from scipy.stats import kstest\n",
    "\n",
    "# Nivel de confianza del 99%\n",
    "alpha = 0.01\n",
    "h_norm = np.zeros(X_train.shape[1])\n",
    "\n",
    "for i in range(0, X_train.shape[1]):\n",
    "     _, pvalue = kstest(X_train[:, i], 'norm')\n",
    "    \n",
    "    # Contraste de hipotesis\n",
    "    id pvalue <= aplha:\n",
    "        # Los datos NO siguen una distribución normal N(0, 1)\n",
    "        h_norm[i] = 0\n",
    "    \n",
    "    else:\n",
    "        # Los datos Si siguen una distribución normal N(0, 1)\n",
    "        h_norm[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c419eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_boxplot(data_1, data_2, ticks):\n",
    "    \n",
    "    bp1 = plt.boxplot(data_1, positions=np.array(range(np.shape(data_1)[1]))*2.0-0.4, sym='', widths=0.5, \\\n",
    "                     boxprops=dict(color='red'),\n",
    "                     capprops=dict(color='red'),\n",
    "                     whiskerprops=dict(color='red'),\n",
    "                     medianprops=dict(colot='red'))\n",
    "    \n",
    "    bp1 = plt.boxplot(data_2, positions=np.array(range(np.shape(data_2)[1]))*2.0+0.4, sym='', widths=0.5, \\\n",
    "                     boxprops=dict(color='blue'),\n",
    "                     capprops=dict(color='blue'),\n",
    "                     whiskerprops=dict(color='blue'),\n",
    "                     medianprops=dict(colot='blue'))\n",
    "                      \n",
    "    plt.plot([], c='#D7191C', label='Glaucoma')\n",
    "    plt.plot([], c='#2C7DB6', label='Healthy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.xticks(range(0, len(ticks)*2, 2), ticks)\n",
    "    plt.xlim(-2, len(ticks)*2)\n",
    "    plt.grid(True)\n",
    "    plt.title('Características')\n",
    "    plt.show()s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estudiar la capacidad discriminativa de los atributos en función de su distribución\n",
    "# Con la ttest_ind hago una comparación de medias y con mannhwhitneyu hago una comparación de medianas \n",
    "from scipy.stats import ttest_ind, mannhwhitneyu\n",
    "\n",
    "glaucoma_data = X_train[y_train == 1]\n",
    "healthy_data = X_train[y_train == 0]\n",
    "\n",
    "h = np.zeros(X_train.sahep[1])\n",
    "h_disc = np.zeros(X_train.shape[1])\n",
    "\n",
    "for i in range(0, X_train.shape[1]):\n",
    "    # Si no es normal --> comparación de medianas\n",
    "    if h_norm[i] == 0:\n",
    "        # Lo que estoy haciendo aqui es comparar las medianas de los valores que toma la característica i\n",
    "        # cuando la muestra es de glaucoma y cuando la muestra es sana\n",
    "        \n",
    "        # A nosotros nos interesa que en este punto las medianas sean diferentes por que eso me indicara que\n",
    "        # hay una dependencia entre la característica y la clase\n",
    "        # Digamos que nos interesa que tome valores diferentes de mediana cuando la clase es de glaucoma\n",
    "        # que cuando la clase es sana ya que esto ayudara en la fase de modelado\n",
    "        # al modelo para distinguir entre una muestra sana y una con glaucoma\n",
    "        _, pvalue = mannhwhitneyu(glaucoma_data[:,i], healthy_data[:,i])\n",
    "        \n",
    "        # Si obtenemos pvalue con valores de mediana iguales, discriminamos ese atributo.\n",
    "    # Si es normal --> comparación de medias\n",
    "    else:\n",
    "        # De igual manera discriminaremos aquellas que la media sea la misma\n",
    "        _, pvalue = ttest_ind(glaucoma_data[:,i], healthy_data[:,i])\n",
    "        \n",
    "    # Contraste de hipotesis, estudiar el poder discriminatorio de las características\n",
    "    # Definimos la hipotesis nula\n",
    "    # H0: Independencia entre la característica y la clase\n",
    "    if pvalue <= aplha:\n",
    "        # Se rechaza la hipotesis nula, y por tanto, asumimos la dependencia entre \n",
    "        # la característica y la clase\n",
    "        h_disc[i] = 1\n",
    "    else:\n",
    "        # No hay evidencia para rechazar la HO, y por tanto, asumimos que la caract. y la clase son independientes\n",
    "        # Tenemos que eliminar estos atributos ya que son independientes de la clase y no aportan\n",
    "        h_disc[i] = 0\n",
    "\n",
    "# Eliminando las variables que no son discriminatorias ya que si las mantengo solo meto ruido al modelo\n",
    "# afectando el rendimiento de nuestro modelo\n",
    "\n",
    "id_no_disc = np.where(h_disc == 0)\n",
    "\n",
    "# Eliminamos de nuestra matriz de datos original los atributos que no discriminan \n",
    "X_train_disc = np.delete(X_train, id_no_disc[0], axis=1)\n",
    "\n",
    "mu_disc = np.delete(mu, id_no_disc[0])\n",
    "sigma_disc = np.delete(sigma, id_no_disc[0])\n",
    "\n",
    "# Visualización para enfrentar el diagrama de caja y bigotes\n",
    "original_ticks = ['media', 'mediana', 'std', 'asim', 'curtosis', 'min', 'max', 'con', 'dis', 'homo', 'ASM',\n",
    "                  'E', 'COR', 'LBP1', 'LBP2', 'LBP3', 'LBP4', 'LBP5', 'LBP6', 'LBP7', 'LBP8', 'LBP9', 'LBP10']\n",
    "\n",
    "draw_boxplot(glaucoma_data, healthy_data, original_ticks)\n",
    "\n",
    "# Eliminamos características que han resultado no ser discriminantes\n",
    "ticks = np.delete(original_ticks, id_no_disc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b366b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una vez que hemos obtenidos que variables tienen capacidad discriminativa \n",
    "# y cuales no, ahora lo que haremos será estudiar de manera cuantitativa que variables están \n",
    "# correladas con otras ya que aportan la misma información al modelo.\n",
    "# Haremos esto para obtener un modelo más robusto en términos de coste computacional\n",
    "\n",
    "# Realizar un análisis de CORRELACIÓN para ver la dependencia entre pares de variables\n",
    "# En este punto nos interesa la INDEPENDENCIA entre variables, ya que queremos eliminar aquellas\n",
    "# variables que aportan la misma información al modelo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Matriz de correlación de variables discriminantes\n",
    "R = np.corrcoef(X_train_disc.transpose())\n",
    "\n",
    "# Esta matriz nos permite observar de manera visual la correlación entre diferentes pares de variables\n",
    "# Mientras más rojo sea el color, quiere decir que hay una correlación más fuerte\n",
    "plt.imshow(R, cmap='jet')\n",
    "plt.show()\n",
    "\n",
    "# Umbral de correlación. Si 2 variables están correlacionadas más de un 90%\n",
    "# entonces podemos pasar a descartar esa variable del estudio\n",
    "th_cor = 0.9\n",
    "\n",
    "# Vamos a declarar un indice en valor absoluto en donde\n",
    "# indicaremos en que lugar ese indice es mayor que el umbral\n",
    "# Lo ponemos en valor absoluto ya que no nos importa si hay una correlación positiva o negativa\n",
    "# solo nos importa si están correladas o no\n",
    "idx = abs(R) > th_cor\n",
    "mat_tri_sup = np.triu(idx, 1)\n",
    "\n",
    "row, cow = np.where(mat_tri_sup == True)\n",
    "id_corr = np.unique(col)\n",
    "\n",
    "# Eliminamos las variables correlacionadas\n",
    "X_final = np.delete(X_train_disc, id_corr, axis=1)\n",
    "mu_final = np.delete(mu_disc, id_corr)\n",
    "sigma_final = np.delete(sigma_disc, id_corr)\n",
    "ticks = np.delete(ticks, id_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3131d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardado de matriz final de características\n",
    "import os\n",
    "\n",
    "if not os.path.exists('../final_features'):\n",
    "    os.mkdir('../final_features')\n",
    "    \n",
    "y_train_exp = np.expand_dims(y_train, axis=1)\n",
    "train_matrix = np.concatenate((X_fianl, y_train_exp), axis=1)\n",
    "np.save('../final_features/train.npy', train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b09129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora prepararemos nuestro set de datos test\n",
    "# repetiremos el proceso para la selección de las características del test\n",
    "\n",
    "test_matrix = np.load('../features/matriz_datos_test.npy')\n",
    "\n",
    "# Seleccionar las características y la clase\n",
    "\n",
    "X_test = test_matrix[:,:-1]\n",
    "y_test = test_matrix[:, -1]\n",
    "\n",
    "# Eliminar las características que no son discriminatorias del conjunto test durante entrenamiento\n",
    "X_test_disc = np.delete(X_test, id_not_disc[0], axis=1)\n",
    "\n",
    "# Ahora eliminamos las características que están correladas durante entrenamiento\n",
    "X_test_fianl = np.delete(X_test_disc, id_corr, axis=1)\n",
    "\n",
    "# Ahora tenemos que estandarizar nuestro set de test en base a la mu y la sigma del entrenamiento\n",
    "# Esto sería al equivalente del .transform del objeto estandarizado con el set de train\n",
    "# sin embargo ya que hemos realizado la estandarización antes de la selección de atributos\n",
    "# hemos tenido que ir arrastrando la mu y la sigma para poder hacer luego el centrado y el escalado\n",
    "X_test_final = (X_test_final-mu_final)/sigma_final\n",
    "\n",
    "y_test_exp = np.expand_dims(y_test, axis=1)\n",
    "test_matrix = np.concatenate((X_test_final, y_test_exp), axis=1)\n",
    "\n",
    "np.save('../final_features/test.npy', test_matrix)\n",
    "\n",
    "# De esta manera queda concluida la fase de selección de características y podemos pasar a la fase de modelado"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
